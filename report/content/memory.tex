\section{Problem size}
One of the main problems when parallelizing the algorithm is that the amount of data in real applications is huge. Images from synchrotrons are generated with detectors of sizes up to $4000\times4000$. To accurately generate 3D reconstructions it has been proven that approximately $\frac{\pi\cdot N}{2}$ vieweing angles are needed, where $N$ is the size of the detector in one direction~\cite{natterer2001}.\\
We will benchmark the algorithms by using sizes $N$ ranging from $128$ to $4096$ and use $\lceil\frac{\pi\cdot N}{2}\rceil$ angles and $N$ lines for each of these sizes.\\
To solve the large problems it is not possible to store the whole system matrix on the GPU. Lets take as an example the amount of space used to store the matrix for a 3D problem with a $N=4000$ detector, with $\lceil\frac{\pi\cdot N}{2}\rceil$ angles, and $4000$ rays per angle. The matrix is sparse having at most $2\cdot N-1$ entries in each row, so instead of storing the whole matrix with all the zeroes, we will consider a semi-sparse representation with one 2 dimensional array containing the data as floats in arrays on each row of size  $2\cdot N-1$, and one matching 2 dimensional array containing the column indexes of the datapoints. For the example sketched above the number of rows of each matrix will be $\lceil\frac{\pi\cdot4000}{2}\rceil\cdot4000$ and the number of columns is $2\cdot 4000-1$. Assuming ints and floats both take up 4 bytes each then storing the matrix requires $4\cdot 2\cdot\lceil\frac{\pi\cdot 4000}{2}\rceil\cdot4000\cdot(2\cdot 4000-1)\approx1TB$. This is of course one of the largest problems, but even for a problem with $N=512$ we're looking at the matrix taking up around $3-4GB$. Besides the matrix we also need to store the sinogram data which will take approximately $420MB$, and the reconstruction taking up around $270MB$. Hence even for relatively small problems we're looking at an estimated $5GB$ of data. GPU memory is expensive, and is typically in the range 2-8GB for consumer end cards, and 10-48GB for high end cards. Thus, the small problems are not able to fit in a standard comsumer card, and large problems won't fit on any cards.\todo{L\ae rke should probably check these calculations...}\\
There are several ways of getting around this problem. Some problems exhibit symmetries in the system matrix so that we only have to store part of the matrix. This is already somewhat accounted for in the above calculations. In the example we only considered the matrix for one slice of the reconstruction since it will be equivalent for other slices when using parallel beam geometry. Thus for cone beam geometries, the memory needed would be approximately $N$ times greater. Other symetries involve angle symmetries, where for example a scan from a $45^{\circ}$ or $135^{\circ}$ angle would contain the same values for mirrored pixels. However these are obviously not generally applicable since they will depend on which angles are used in the reconstruction.\\
Another more flexible approach is to calculate the matrix values on the fly which is what we have done. For this we used the code from a bachelor project in which they did an implementation of finding the system matrix using futhark. We had two different parallel implementations in futhark and a sequential version in pyhton which we compared\todo{do comparison and insert comparison of versions with a graph showing the performance of the two, vs. francois version}. We decided to use the fastest version XXX \todo{fill out which one we used and why and some thoughts on why one is faster than the other}\\
