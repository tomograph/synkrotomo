\section{Motivation}
This section will discuss the motivation behind implementing the algorithm for this project on the GPU. First a general intuition of GPU computing is provided, as well as a description of two important notions in parallel computing: \emph{Task independence} and \emph{Degree of parallelization}. Later the specifics of this project are treated in this context.

% The goal of this section is to gain a general sense of GPU computing. 
\subsection{GPU computing}
What follows is a brief introduction to GPU computing. The GPU contains numerous computational units, often referred to as \emph{kernels}. Each kernel can be thought of as a small and very slow CPU-core on a multi-threaded CPU, although there are some key differences. Most importantly there are often tens of thousands of kernels on a modern GPU, while a typical CPU has 4 or 8 cores. Each kernel is significantly slower than a CPU-core, but kernels have the advantage of being able to run in parallel. The following analogy should provide some motivation as to why, and when, parallel execution like this is useful.

Consider a common and somewhat compute intensive task, like for instance a matrix-vector multiplication. 
Now for arguments sake imagine that some CPU might be able to complete this task in 1 unit of time while a GPU kernel would take 100 of those units. Now imagine that it needs to perform the task 10.000 times and importantly that each task is not dependent on the result of any of the other tasks. The CPU would simply take 10.000 units of time to complete this computation, one per task. The GPU however, is able to instantiate 10.000 kernels and run them simultaneously. Each kernel takes a 100 units of time to finish, but they all run in parallel so the total computation time is only a 100 units of time.

It is easy to be tempted to thinking that every task should just be done on the GPU when seeing a (highly constructed) example such as the one above. In reality there is only a subset of computational tasks that are a good fit for parallel computation. There are two crucial attributes that need consideration when trying to assess whether there is potential performance gains gains by utilizing the GPU for a specific task. These are:

\textbf{Task independence:} As mentioned briefly in the previous example it is paramount that the tasks are not interdependent. That is, if the result of the i'th kernel depends on the result of the j'th kernel, then the i'th kernel cannot run in parallel with the j'th. The worst case is a task that has an iterative nature, where one or more variables are updated a number of times via their previous values. A short-hand way to understand this requirement if you are a functional programmer is this: You must be able to construct an array of input values and then transform these, by mapping a pure function over this array, to produce the array of the desired results.

\textbf{Degree of parallelization:} To explain this concept let us return to our analogy:
Instead of performing the task 10.000 times, consider the case where we only do it a single time. Then the CPU would simply take 1 unit of time. But the GPU would take 100 units in this case since it has to run a single slow kernel to do the computation. So the tables have turned and it is now the GPU that is a hundred times slower. This illustrates that the degree of parallelization is crucial to gain a performance boost. If the amount of independent tasks is too low, then parallel execution is likely to be as slow or often even slower than sequential execution. The short-hand for programmers in this case is that the array of inputs must have a significant size. We return to this size in the next section.

With this intuition we are ready to analyse the degree of parallelization in our problem, to assess whether the GPU should theoretically be suitable in our situation. Do note however that there are many details that we skipped in this section. For instance we have not considered the overhead from kernel-instantiation nor the impact that the IO operations between the CPU and the GPU have. These things are definitely something that one should worry about in a real life setting. We refer to section \ref{sec:bench} on benchmarking for more details on this topic.
%TODO: check whether theres something to refer to here.

\subsection{Degree of parallelization in ART} 
\label{sec:degOfPar}
Anecdotally we require at least $30.000$ parallel threads, for GPU-computing to be worthwhile\footnote{Based on discussions with Postdoc Troels Henriksen.}. To answer if our problem is well suited for parallelization, we should look at its attributes. These are the number of lines per scan, the number of angle increments per scan and the size of the grid that the lines intersect with. For the sake of this discussion let us fix each of these attributes to a realistic value. The grid can be 3000 by 3000, the number of lines per scan can be 2000 and the number of angle increments can be 2000 as well. It is clear that we cannot run a kernel per angle increment as 2000 kernels is far too few. Instead we should spawn a kernel per line per angle increment. The intersection lengths of each line is not dependent on any result from any other line, so we have independent tasks. As for the degree of parallelization, in this case we have $3000 * 2000 = 6.000.000$ which is more than enough, and in fact is probably larger than what can fit in the GPU's memory.

% we note that for each angle, around 2000 independent rays are sent out. Each of these rays, can have up to $2*grid\_size-1$ independent intersections with the grid, meaning that with a 3000 by 3000 grid we have $2000 * 3000 = 6.000.000$ independent calculations. Add to that, that we need to perform these calculations for around $1000$ different angles, we end up with $1.080.000.000$ independent calculations to be performed. This number is far above the required $30.000$ threads, but requires that we calculate each intersection completely independently, and with a minimum of branching logic.
